# Added Note
* 中间件(下载器中间件 & 爬虫中间件)
* 下载器中间件
> 更换代理IP，更换Cookies，更换User-Agent，自动重试
* 开发实现
> 中间件本身是Python的一个类，只要爬虫每次访问网站之前都先“经过”这个类，它就能给请求换新的代理IP，这样就能实现动态改变代理。
> 在创建一个Scrapy工程以后，工程文件夹下会有一个middlewares.py文件
> 名字后面的s表示复数，说明这个文件里面可以放很多个中间件。
> Scrapy自动创建的这个中间件是一个**爬虫中间件**

* Singleton Pattern 单例模式
> 某一个类只有一个实例存在
* pytesseract
> 功能是识别图片文件中文字，并作为返回参数返回识别结果；
> 默认支持tiff、bmp格式图片，只有在安装PIL(Python Imaging Library)之后，才能支持jpeg、gif、png等其他图片格式；
* selenium - 浏览器自动化测试框架
> 测试脚本执行时，浏览器自动按照脚本代码做出点击，输入，打开，验证等操作，就像真实用户所做的一样，从终端用户的角度测试应用程序。
* Lambda函数
> 匿名函数，即没有具体名称的函数，它允许快速定义单行函数，类似于C语言的宏，可以用在任何需要函数的地方。这区别于def定义的函数。 
> 用些函数如果只是临时一用，而且它的业务逻辑也很简单，没必要非给它去个名字
> def创建的方法是有名称的，而lambda没有。 
> lambda会返回一个函数对象，但这个对象不会赋给一个标识符，而def则会把函数对象赋值给一个变量（函数名）。 
> lambda只是一个表达式，而def则是一个语句。
> lambda表达式” : “后面，只能有一个表达式，def则可以有多个。 
> 像if或for或print等语句不能用于lambda中，def可以。 
> 定义简单的函数
> 不能共享给别的程序调用
> 语法格式：
> lambda [arg1 [,agr2, .... argn]]:expression 
> 单个参数的：
> g = lambda x: x ** 2
> print g(3)
> 多个参数的：
> g = lambda x,y,z :(x+y) **z
> print g(1,2,2)
> 
> 如果定义匿名函数，还要给它绑定一个名字的话，有点画蛇添足，通常是直接使用 lambda 函数。那么 lamdba 函数的正确使用场景在哪呢？
> 1 - 函数式编程
> 尽管 Python 算不上是一门纯函数式编程语言，但它本身提供了很多函数式编程的特性，像 map、reduce、filter、sorted 这些函数都支持函数作为参数，lambda 函数就可以应用在函数式编程中。
> 
> EXP.
> 
> list1 = [3,5,-4,-1,0,-2,-6]
> sorted(list1,key= lambda x:abs(x))
> 
> 2 - 闭包
> 在这里我们可以简单粗暴地理解为闭包就是一个定义在函数内部的函数，闭包使得变量即使脱离了该函数的作用域范围也依然能被访问到。
> 
> EXP. 
> 
> def my_add(n):
>   return lambda x:x+n 
>   
> add_3 = my_add(3)
> add_3(7)
> 
> 在爬虫中的应用
> https://blog.csdn.net/weixin_40982642/article/details/78583560
> https://zhuanlan.zhihu.com/p/27607704
> 
> K-means聚类算法
> 
> 1 - 确定聚类的个数k
> 
> 2 - 选定k的D维向量作为初始类的中心
> 
> 3 - 对每个样本计算与聚类中心的距离，选择最近的作为该样本所属的类
> 
> 4 - 在同一类内部，重新计算聚类中心（几何重心） 不断迭代，直到收敛。 
> 
> KNN近邻算法：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。
* 处理验证码问题
> 自动识别验证码
> 重新请求 （前提是使用了代理IP）
* 重试中间件
> 有时使用代理会被远程拒绝或超时错误，这时需要换代理IP重试
> 重写 scrapy.downloadrmiddlewares.retry.RetryMiddleware
* Scrapy介绍
> 包含爬取数据、提取结构性数据、应用框架
> 
> 底层通过Twisted异步网络框架处理网络通讯
> 
> 可扩展、高性能、多线程、分布式爬虫框架
* 常用命令
> startproject 创建一个新项目
> 
> genspider 根据模板生成一个新爬虫
> 
> crawl 执行爬虫
> 
> shell 启动交互式抓取控制台
* scrapy的去重原理
> 原理：
> 
> 对于每一个url的请求，调度器都会根据请求得相关信息**加密**得到一个**指纹信息**，并且将指纹信息和set()集合中的指纹信息进行比对，如果set()集合中已经存在这个数据，就不在将这个Request放入队列中。如果set()集合中没有存在这个加密后的数据，就将这个Request对象放入队列中，等待被调度。
> 
> 如何操作？
> 
> dont_filter默认为False,即开启去重;
* 用过什么中间件？
> 
> 下载器中间件：
> 
> 主要使用下载中间件处理请求，一般会对请求设置随机的User-Agent ，设置随机的代理。目的在于防止爬取网站的反爬虫策略。
> 
> 引擎将请求传递给下载器过程中， 下载中间件可以对请求进行一系列处理。比如设置请求的 User-Agent，设置代理等
> 
> 在下载器完成将Response传递给引擎中，下载中间件可以对响应进行一系列处理。比如进行gzip解压等。
> 
> 爬虫中间件（没用过）：
> 
> 用法与下载器中间件非常相似，只是它们的作用对象不同。下载器中间件的作用对象是请求request和返回response；爬虫中间件的作用对象是爬虫，更具体地来说，就是写在spiders文件夹下面的各个文件。
> 
> 在中间件处理爬虫本身的异常
> 
> 
> 
> 实际操作：
> 
> 在setting.py中间设置好，随机的user-agent和proxy 
> 
> 创建middlewares.py来新建中间件
* 爬虫为什么用到代理？
> 如果同一个IP频繁的抓取某个网站，很容易造成IP被封
> 代理IP质量好，速度快，就能提高爬取效率；代理IP干净，使用的人少，就不会被反爬虫策略发现，成功率就高。]
> 
> 
* 怎么使用代理？
> 把代理的信息放在header里面
> `i_headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.48'}
`
> `req = urllib2.Request(new_url, headers=i_headers)`

> px = request.ProxyHandler({'http':'0.0.0.0'})
> opener = request.build_opener(px)
> req = request.Request('网址')
> res = opener.open(req)
> request.install_opener(opener)
> res = request.urlopen(req)
> 
* 代理失效问题如何解决？
> process_request 在请求之前会被调用，来设置代理
> 
> process_exception 在请求失败的时候被调用，用来判断代理失效
> 
> 
> 自己编写下载器中间件，scrapy每次连接失败都会重新执行激活的中间件
>  
* 登陆验证码问题：
> 开源的OCR库 (Optical Character Recognition)
> 验证码截图人工识别
> 或者找第三方公司
* 爬取速度过快带来的验证码处理
> 换一个IP， 慢一点
* 模拟登陆
> 为什么有这个需求？
> 很多网站都必须要登陆权限，爬虫如果想获取有用的信息就必须带有登陆后的cookie。
> 
> 社交性质的网站只有用户登陆之后，才会出现有价值的内容
> 
> 在手动登陆的时候有个小技巧，那就是故意把密码填错，这样可以很容易看到用户名和密码正确的提交路径和提交方式。
> 
> Python中的cookielib库
> 
> 
* 构建分布式爬虫是时考虑的问题：
> 如何能保证多台机器同时抓取同一个URL？
> 
> 如果某个节点挂掉，会不会影响其它节点，任务如何继续？
> 
> 既然是分布式，如何保证架构的可伸缩性和可扩展性？不同优先级的抓取任务如何进行资源分配和调度？
> 
> celery 作为分布式调度工具
> 
> 会把所有任务都通过消息队列发送给各个分布式节点进行执行，所以可以很好的保证url不会被重复抓取；
> 
> 它在检测到worker挂掉的情况下，会尝试向其他的worker重新发送这个任务信息
> 
> 
> 
* 解决编码问题
> encode() decode()